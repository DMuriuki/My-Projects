---
title: "R Notebook"
output: html_notebook
---
```{r}
# Define an  exponential function
expo<-function(x,y){
  result<-x^y
  return(result)
}
# Call expo function with arguments 3 and 4
expo(x=3,y=4)
```

```{r}
#Loading data set into R
library(data.table)
Princeton<-read.table("https://data.princeton.edu/wws509/datasets/salary.dat", header = TRUE)
str(Princeton)
```


```{r}
Princeton$sx <- with(Princeton, ifelse(sx == "male", 0, 
                              ifelse(sx == "female", 1, NA)))

Princeton$rk <- with(Princeton, ifelse(rk == "assistant", 1, 
                              ifelse(rk == "associate", 2,
                              ifelse(rk == "full", 3,NA))))
                    
Princeton$dg <- with(Princeton, ifelse(dg == "doctorate", 1, 
                              ifelse(dg == "masters", 0,NA)))
head(Princeton)
```

```{r}

#Setting seed to reproduce results of random sampling
set.seed(123)
train_index <- sample(1:nrow(Princeton), 0.8 * nrow(Princeton))
trainingData <- Princeton[train_index, ]  # model training data
testData  <- Princeton[-train_index, ]# model testing data

```

```{r}
model.fit<- lm(sl~sx+rk+yr+dg+yd, data=trainingData)  # build the model
summary(model.fit)
```

```{r}
install.packages("psych")
library(psych)
pairs.panels(trainingData[c("sl","sx","rk","yr","dg","yd")])
```

```{r}
plot(model.fit)
```

```{r}
#Choose the best linear model by using step()
stpModel=step(lm(data=Princeton, sl~.), trace=0, steps=1000)
summary(stpModel)
```

```{r}
#we use rank and year in current rank as our new model to make our prediction analysis
x1<-trainingData$rk
x2<-trainingData$yr
y<-trainingData$sl
newmodel<-lm(y~x1+x2)
summary(newmodel)


```

```{r}
x1<-trainingData$rk
x2<-trainingData$yr
y<-trainingData$sl
newmodel<-lm(y~x1+x2)
summary(newmodel)
```

```{r}
#Define our Test data frame
Test<-data.frame(x1=testData$rk, x2=testData$yr)
Test
```

```{r}
#Predict our new model based on the testing data.
Pred<-predict(newmodel,Test)
testData$prediction<-Pred
print(testData)

```

```{r}
sales <- read.csv("http://ucanalytics.com/blogs/wp-content/uploads/2017/09/Data-L-Reg-Gradient-Descent.csv")
sales$X1plusX2 <- NULL
var_names <- c("expenditure", "income", "purchase")
names(sales) <- var_names
head(sales)
```

```{r}
#Setting seed to reproduce results of random sampling
set.seed(5689)
train_index <- sample(1:nrow(sales), 0.75 * nrow(sales))
trainingData <- sales[train_index, ]  # model training data
testData  <- sales[-train_index, ]# model testing data
glm_fit<- glm(purchase~ income + expenditure, data= trainingData, family = binomial)
summary(glm_fit)
coef(glm_fit)
```

```{r}
glm_predict<-predict(glm_fit, newdata=testData, type="response")
glm_predict
y_predict <- ifelse(glm_predict>.5, 1, 0)

y_observed <- testData$purchase

table(y_predict, y_observed)


```

```{r}
#Data Import and Preparation
sales <- as.data.frame(read.csv("http://ucanalytics.com/blogs/wp-content/uploads/2017/09/Data-L-Reg-Gradient-Descent.csv"))
#We'll employ MLE to find the optimal values/ regression coefficients
#First we Create the design matrices
#Predictor variables
x<- as.matrix(sales[,c(1,2)])
#Add ones to x
x<- cbind(rep(1,nrow(x)),x)
#Predicted variable
y<- sales$purchase
#Response variable matrix
y<- as.matrix(sales$y)
```

```{r}
# define the log-likelihood function
logl <- function(theta,x,y){
    y <- y
    x <- as.matrix(x)
    beta <- theta[1:ncol(x)]
    loglik <-  sum(-y*log(1 + exp(-(x%*%beta))) - (1-y)*log(1 + exp(x%*%beta)))
    return(-loglik)
}
# Define initial values for the parameters
theta.start = rep(0,3)
names(theta.start) = colnames(x)
# Calculate the maximum likelihood
mle = optim(theta.start,logl,x=x,y=y,hessian=F)
# Obtain regression coefficients
beta = mle$par
beta

```

```{r}
#Sigmoid Function

sigmoid <- function(z){
  g <- 1/(1+exp(-z))
  return(g)
}
```

```{r}
#Cost Function with beta parameters
logistic_cost_function <- function(beta){
  m <- nrow(X)
  g <- sigmoid(X%*%beta)
  J <- (1/m)*sum((-Y*log(g)) - ((1-Y)*log(1-g)))
  
  return(J)
}
```

```{r}
# Define learning rate and iteration limit
alpha <- 0.0005
num_iters <- 10000

# initialize coefficients
beta <- matrix(c(0,0,0), nrow=3)

# gradient descent
for (i in 1:num_iters) {
  error <- (sigmoid(x%*%beta) - y)
  delta <- (t(x) %*% error) / length(y)
  beta <- beta - (alpha*delta)
}

```

```{r}
l<-glm(y~x1+x2,family = 'binomial',data=sales)
beta
l$coefficients
```



This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
